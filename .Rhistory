group_by(id, author_id, commenter_id) %>%
summarise(count=sum(count)) %>%
arrange(desc(count))
rel_commenter_df2 <- rel_commenter_df1 %>%
group_by(author_id, commenter_id) %>%
summarise(count=n_distinct(count)) %>%
select(commenter_id, author_id, count) %>%
arrange(desc(count))
print(paste("發文數：", length(unique(rel_commenter_df1$id))))
print(paste("發文者人數：", length(unique(rel_commenter_df2$author_id))))
print(paste("推文者人數：", length(unique(rel_commenter_df2$commenter_id))))
# 將data frame轉換成graph，以推文者回應發文者的發文數為兩個連結間的權重
post_graph <- graph_from_data_frame(rel_commenter_df2)
# 參與的使用者帳號：Number of Vertices, vcount
print(paste("參與的使用者帳號數：", vcount(post_graph)))
# 推文者回應的發文者帳號數
out_deg <- degree(post_graph, v = V(post_graph), mode = "out",
loops = TRUE, normalized = FALSE)
table(out_deg)
# 推文者回應的發文者帳號數：0，只參與發文的使用者帳號
print(paste("只參與發文的使用者帳號數：", length(which(out_deg==0))))
# 發文者獲得的推文者帳號數
in_deg <- degree(post_graph, v = V(post_graph), mode = "in",
loops = TRUE, normalized = FALSE)
table(in_deg)
# 發文者獲得的推文者帳號數：0，只參與推文的使用者帳號
print(paste("只參與發文的使用者帳號數：", length(which(in_deg==0))))
# 同時有發文與推文的帳號數
print(paste("同時有發文與推文的帳號數：",
vcount(post_graph)-length(which(out_deg==0))-length(which(in_deg==0))))
user_df <- data.frame(user_id= V(post_graph), in_deg, out_deg,
stringsAsFactors = FALSE)
V(post_graph)[1:10]
str(V(post_graph)[1:10])
str(V(post_graph)[1:10]$names)
attr(V(post_graph), "names")[1:10]
user_df <- data.frame(user_id= attr(V(post_graph), "names"),
in_deg, out_deg,
stringsAsFactors = FALSE)
View(user_df)
View(user_df)
user_df %>%
filter(in_deg>0 & out_deg>0) %>%
ggplot(aes(x=out_deg, y=in_deg)) +
geom_point()
user_df1 <- user_df %>%
filter(in_deg>500 | out_deg>50)
View(user_df1)
user_df1 <- user_df %>%
filter(in_deg>0 & out_deg>0) %>%
filter(in_deg>250 | out_deg>50)
View(user_df1)
user_df %>%
filter(in_deg>0 & out_deg>0) %>%
ggplot(aes(x=out_deg, y=in_deg)) +
geom_point() +
geom_text(aes(x=out_deg+5, y=in_deg, label=user_id),
data=user_df1)
View(user_df1)
user_df %>%
filter(in_deg>0 & out_deg>0) %>%
ggplot(aes(x=out_deg, y=in_deg)) +
geom_point() +
geom_text(aes(x=out_deg+5, y=in_deg, label=user_id),
data=user_df1) +
labs(x="回應帳號數", y="受回應帳號數")
med_out <- median(user_df$out_deg)
med_in <- median(user_df$in_deg)
?switch
case( c(1,1,4,3), "a"=1, "b"=2, "c"=3)
user_df %<>%
mutate(category=case_when(
in_deg<=med_in & out_deg<=med_out ~ "不活躍使用者",
in_deg<=med_in & out_deg>med_out ~ "偏向回應使用者",
in_deg>med_in & out_deg<=med_out ~ "偏向發言使用者",
in_deg>med_in & out_deg>med_out ~ "靈魂人物"
))
View(user_df)
post_df <- read_excel("TaiwanDrama.xlsx") %>%
mutate(comment_no=as.integer(comment_no),
push_no=as.integer(push_no),
commenter_no=as.integer(commenter_no)) %>%
mutate(post_date=as.Date(post_date)) %>%
mutate(rel=grepl("與惡", title)|grepl("與惡", ptext))
commenter_df <- read_excel("TaiwanDrama_commenters.xlsx")
rel_commenter_df <- commenter_df %>%
filter(id %in% post_df$id[post_df$rel]) %>%
group_by(id, author_id, commenter_id, comment_label) %>%
summarise(count=n())
rel_commenter_df1 <- rel_commenter_df %>%
group_by(id, author_id, commenter_id) %>%
summarise(count=sum(count)) %>%
arrange(desc(count))
rel_commenter_df2 <- rel_commenter_df1 %>%
group_by(author_id, commenter_id) %>%
summarise(count=n_distinct(count)) %>%
select(commenter_id, author_id, count) %>%
arrange(desc(count))
print(paste("發文數：", length(unique(rel_commenter_df1$id))))
print(paste("發文者人數：", length(unique(rel_commenter_df2$author_id))))
print(paste("推文者人數：", length(unique(rel_commenter_df2$commenter_id))))
# 將data frame轉換成graph，以推文者回應發文者的發文數為兩個連結間的權重
post_graph <- graph_from_data_frame(rel_commenter_df2)
# 參與的使用者帳號：Number of Vertices, vcount
print(paste("參與的使用者帳號數：", vcount(post_graph)))
# 推文者回應的發文者帳號數
out_deg <- degree(post_graph, v = V(post_graph), mode = "out",
loops = TRUE, normalized = FALSE)
table(out_deg)
# 推文者回應的發文者帳號數：0，只參與發文的使用者帳號
print(paste("只參與發文的使用者帳號數：", length(which(out_deg==0))))
# 發文者獲得的推文者帳號數
in_deg <- degree(post_graph, v = V(post_graph), mode = "in",
loops = TRUE, normalized = FALSE)
table(in_deg)
# 發文者獲得的推文者帳號數：0，只參與推文的使用者帳號
print(paste("只參與發文的使用者帳號數：", length(which(in_deg==0))))
# 同時有發文與推文的帳號數
print(paste("同時有發文與推文的帳號數：",
vcount(post_graph)-length(which(out_deg==0))-length(which(in_deg==0))))
user_df <- data.frame(user_id= attr(V(post_graph), "names"),
in_deg, out_deg,
stringsAsFactors = FALSE)
user_df1 <- user_df %>%
filter(in_deg>0 & out_deg>0)
med_in <- median(user_df1$in_deg)
med_out <- median(user_df1$out_deg)
user_df1 %<>%
mutate(category=case_when(
in_deg<=med_in & out_deg<=med_out ~ "不活躍使用者",
in_deg<=med_in & out_deg>med_out ~ "偏向回應使用者",
in_deg>med_in & out_deg<=med_out ~ "偏向發言使用者",
in_deg>med_in & out_deg>med_out ~ "靈魂人物"
))
user_df2 <- user_df1 %>%
filter(in_deg>250 | out_deg>50)
user_df1 %>%
ggplot(aes(x=out_deg, y=in_deg)) +
geom_point() +
geom_text(aes(x=out_deg+5, y=in_deg, label=user_id),
data=user_df2) +
labs(x="回應帳號數", y="受回應帳號數")
user_df1 %>%
ggplot(aes(x=out_deg, y=in_deg)) +
geom_point(aes(color=category)) +
geom_text(aes(x=out_deg+5, y=in_deg, label=user_id),
data=user_df2) +
labs(x="回應帳號數", y="受回應帳號數")
med_in <- median(user_df1$in_deg)*2
med_out <- median(user_df1$out_deg)*2
user_df1 %<>%
mutate(category=case_when(
in_deg<=med_in & out_deg<=med_out ~ "不活躍使用者",
in_deg<=med_in & out_deg>med_out ~ "偏向回應使用者",
in_deg>med_in & out_deg<=med_out ~ "偏向發言使用者",
in_deg>med_in & out_deg>med_out ~ "靈魂人物"
))
user_df1 %>%
ggplot(aes(x=out_deg, y=in_deg)) +
geom_point(aes(color=category)) +
geom_text(aes(x=out_deg+5, y=in_deg, label=user_id),
data=user_df2) +
labs(x="回應帳號數", y="受回應帳號數")
post_df %>%
filter(rel==TRUE) %>%
filter(author=="vk64sg13") %>%
select(title, ptext)
post_df %>%
filter(rel==TRUE) %>%
filter(author=="vk64sg13") %T>%
print(title) %T>
print(ptext)
post_df %>%
filter(rel==TRUE) %>%
filter(author=="vk64sg13") %T>%
print(title) %T>%
print(ptext)
post_df %>%
filter(rel==TRUE) %>%
filter(author=="vk64sg13") %T>%
print(title)
post_df %>%
filter(rel==TRUE) %>%
filter(author %in% c("sodabubble", "ttnakafzcm"))
?gather
post_df %>%
filter(rel==TRUE) %>%
filter(author %in% c("sodabubble", "ttnakafzcm")) %>%
select(author, post_date, comment_no, commenter_no, push_no) %>%
gather(int_type, value, -author, -post_date) %>%
ggplot() +
geom_point(aes(x=post_date, y=value, color=author)) +
facet_wrap(~int_type, nrow=3)
post_df %>%
filter(rel==TRUE) %>%
filter(author %in% c("sodabubble", "ttnakafzcm")) %>%
select(author, post_date, comment_no, commenter_no, push_no) %>%
gather(int_type, value, -author, -post_date) %>%
ggplot() +
geom_point(aes(x=post_date, y=value, color=author)) +
facet_wrap(~int_type, nrow=3, scales="free")
post_df %>%
filter(rel==TRUE) %>%
filter(author %in% c("sodabubble", "ttnakafzcm")) %>%
select(author, post_date, comment_no, commenter_no, push_no) %>%
gather(int_type, value, -author, -post_date) %>%
ggplot() +
geom_point(aes(x=post_date, y=value, color=author)) +
scale_x_date(date_breaks="1 week") +
facet_wrap(~int_type, nrow=3, scales="free")
post_df %>%
filter(rel==TRUE) %>%
mutate(author1=ifelse(author%in%c("sodabubble", "ttnakafzcm"), author, "others")) %>%
select(author1, post_date, comment_no, commenter_no, push_no) %>%
gather(int_type, value, -author1, -post_date) %>%
ggplot() +
geom_point(aes(x=post_date, y=value, color=author1)) +
scale_x_date(date_breaks="1 week") +
facet_wrap(~int_type, nrow=3, scales="free")
post_df %>%
filter(rel==TRUE) %>%
mutate(author1=ifelse(author%in%c("sodabubble", "ttnakafzcm"), author, "others")) %>%
select(author1, post_date, comment_no, commenter_no, push_no) %>%
gather(int_type, value, -author1, -post_date) %>%
ggplot() +
geom_point(aes(x=post_date, y=value, color=author1), alpha=0.3) +
scale_x_date(date_breaks="1 week") +
facet_wrap(~int_type, nrow=3, scales="free")
library(tidyverse)
library(readr)
library(magrittr)
library(tidytext)
fn <- "https://ulist.moe.gov.tw/Download/StartDownload?FileName=108ulistTeacher.csv"
cn <- readLines(fn, 9)
cn1 <- unlist(strsplit(cn[7:9], ","))
cn1 <- iconv(cn1, "cp950", "UTF-8")
df <- read_csv(fn, skip=2)
colnames(df) <- c(cn1[1:13], substr(paste0(cn1[14], cn1[15]), 2, 5),
substr(paste0(cn1[16], cn1[17]), 2, 5), cn1[18:19])
df %<>% mutate_all(iconv, "cp950", "UTF-8") %>%
mutate(教師專長=strsplit(教師專長, "")) %>%
mutate(教師專長=map_chr(教師專長, paste0, collapse=" "))
cn <- readLines(fn, 9)
fn <- "http://ulist.moe.gov.tw/Download/StartDownload?FileName=108ulistTeacher.csv"
cn <- readLines(fn, 9)
fn <- "https://ulist.moe.gov.tw/Download/StartDownload?FileName=108ulistTeacher.csv"
tinytex:::is_tinytex()
install.packages("bookdown")
1:3 %>% accumulate(`+`)
library(magrittr)
library(tidyverse)
1:3 %>% accumulate(`+`)
1:3 %>% accumulate(c)
1:10 %>% accumulate(max, .init = 5)
1:10 %>% accumulate(~ .x)
1:10 %>% accumulate(~ .y)
1:10 %>% accumulate(max ~ .y)
1:10 %>% accumulate(~ .y)
?rnorm
data.frame(idx=1:10, value=rnorm(10))
test <- data.frame(idx=1:10, value=rnorm(10))
test <- test %>%
mutate(long_list=accumulate(value, c))
View(test)
test <- rerun(5, rnorm(100)) %>%
set_names(paste0("sim", 1:5))
View(test)
test <- rerun(5, rnorm(100)) %>%
set_names(paste0("sim", 1:5)) %>%
map(~ accumulate(., ~ .05 + .x + .y))
View(test)
test <- rerun(5, rnorm(100)) %>%
set_names(paste0("sim", 1:5))
test <- test %>%
map(~ accumulate(., ~ .05 + .x + .y))
test <- test %>%
map(~ accumulate(., ~ .05))
test <- test %>%
map(~ accumulate(., ~ .05))
test <- test %>%
map(~ accumulate(., ~ .05))
test <- test %>%
map(~ accumulate(., ~ .05+x))
test <- test %>%
map(~ accumulate(., ~ .05+.x))
test <- test %>%
map(~ accumulate(., ~ .05+.x +.y))
?sample
sample(1:10, 10)
library(tidyverse)
test <- data.frame(idx=1:10, value=sample(1:10, 10))
View(test)
test <- test %>%
mutate(alist=accumulate(value, c)) %>%
mutate(blist=alist[alist<=idx])
test <- test %>%
mutate(alist=accumulate(value, c)) %>%
mutate(blist=alist<=idx)
test <- test %>%
mutate(alist=accumulate(value, c)) %>%
mutate(blist=map(alist, function(x){x<.$idx}))
test$alist[1]
View(test)
test <- test %>%
mutate(alist=accumulate(value, c)) %>%
mutate(blist=map2(alist, idx, function(x, y){x<=y}))
View(test)
View(test)
test <- test %>%
mutate(alist=accumulate(value, c)) %>%
mutate(blist=map2(alist, idx, function(x, y){which(x<=y)}))
View(test)
test <- test %>%
mutate(alist=accumulate(value, c)) %>%
mutate(blist=map2(alist, idx, function(x, y){length(which(x<=y))}))
View(test)
test <- data.frame(idx=1:100, value=sample(1:100, 100))
test <- test %>%
mutate(alist=accumulate(value, c)) %>%
mutate(blist=map2(alist, idx, function(x, y){length(which(x<=y))}))
View(test)
mean <- 7.5
mu <- 6.75
sd <- 2.5
n <- 200
z <- (mean-mu)/(sd/sqrt(n))
p <- 2*(1-pnorm(z))
mean <- 7.25
mu <- 6.5
sd <- 2.5
n <- 200
z <- (mean-mu)/(sd/sqrt(n))
mean <- 7.25
mu <- 6.5
mu <- 6.7
sd <- 2.5
n <- 200
z <- (mean-mu)/(sd/sqrt(n))
p <- 2*(1-pnorm(z))
?z.test
?z.test
?pnorm
z.test <- function (mean_x, mu, sd, samp_size) {
z.score <- (mean_x-mu)/(sd/sqrt(samp_size))
2*(1-pnorm(z.score))
}
z.test(7.25, 6.7, 2.5, 200)
x_vec <- c(1.26, 1.19, 1.31, 0.97, 1.81, 1.13, 0.96, 1.06,
1.00, 0.94, 0.98, 1.10, 1.12, 1.03, 1.16, 1.12,
1.12, 0.95, 1.02, 1.13, 1.23, 0.74, 1.50, 0.50,
0.59, 0.99, 1.45, 1.24, 1.01, 2.03, 1.98, 1.97,
0.91, 1.22, 1.06, 1.11, 1.54, 1.08, 1.10, 1.64,
1.70, 2.37, 1.38, 1.60, 1.26, 1.17, 1.12, 1.23,
0.82, 0.86)
z.test(mean(x_vec), 1.35, sd(x_vec), length(x_vec))
mean(x_vec)
z.score <- abs(mean_x-mu)/(sd/sqrt(samp_size))
z.test <- function (mean_x, mu, sd, samp_size) {
z.score <- abs(mean_x-mu)/(sd/sqrt(samp_size))
(1-pnorm(z.score))
}
z.test(mean(x_vec), 1.35, sd(x_vec), length(x_vec))
z.test <- function (mean_x, mu, sd, samp_size) {
z.score <- (mean_x-mu)/(sd/sqrt(samp_size))
(1-pnorm(abs(z.score)))
}
z.test(mean(x_vec), 1.35, sd(x_vec), length(x_vec))
x_vec <- c(22.6, 26.6, 23.1, 23.5, 27.0, 25.3, 28.6, 24.5,
26.2, 30.4, 27.4, 24.9, 25.8, 23.2, 26.9, 26.1,
22.2, 28.1, 24.2, 23.6)
shapiro.test(x_vec)
?ks.test
ks.test(x_vec)
ks.test(x_vec, pnorm)
ks.test(x_vec, pnorm, mean(x_vec))
ks.test(x_vec, pnorm, mean(x_vec), sd(x_vec))
?t.test
t.test(x_vec, alternative="two.sided", mu=25)
n <- 550
p <- 115/n
z <- (p-pi0)/sqrt(pi0*(1-pi0)/n)
pi0 <- 0.17
z <- (p-pi0)/sqrt(pi0*(1-pi0)/n)
pnorm(z)
1-pnorm(z)
pnorm(z)
1-pnorm(-z)
pnorm(-z)
p <- 706/1009
sqrt(p*(1-p)/1009)
p<-482/1025
sqrt(p*(1-p)/1025)
x<-sqrt(p*(1-p)/1025)
p-2*x
p+2*x
x<-sqrt(p*(1-p)/1009)
p <- 706/1009
x<-sqrt(p*(1-p)/1009)
x.2 <- x/2
p*(1-p)/(x.2^2)
pnorm(3.1)
pnorm(-1.47)
pnorm(-1.06)
install.packages("gapminder")
library(gapminder)
library(tidyverse)
data("gapminder")
View(gapminder)
df <- filter(gapminder, country=="South Africa" | country=="Ireland") %>%
select(country, lifeExp)
View(df)
df <- filter(gapminder, country=="South Africa" | country=="Ireland") %>%
select(country, lifeExp, year)
View(df)
library(tidyverse)
library(readr)
library(magrittr)
library(jiebaR)
library(tm)
library(slam)
library(stm)
library(xlsx)
library(readxl)
news.df <- read_csv(file="parade_text.csv")
setwd("Github/text_analysis")
news.df <- read_csv(file="parade_text.csv")
# 設定jieba斷詞器
mp.seg <- worker(type="mp", user="./parade.dict", bylines=TRUE)
# 製作過濾去不包含中文字或只有一個中文字之候選詞語的函數
filterChineseTerms <- function (str_text) {
str_text <- unlist(str_text)
str_text <- str_text[grepl("\\p{Han}+", str_text, perl=TRUE)]
str_text <- str_text[nchar(str_text)>1]
paste(str_text, collapse=" ")
}
# 將新聞內容斷詞
news.df <- news.df %>%
filter(!is.na(text)) %>%                # 保留有內容的新聞資料
mutate(id=row_number()) %>%             # 每則新聞加上編號(id)
rowwise() %>%                           # 逐行運算
mutate(words=segment(text, mp.seg)) %>% # 斷詞
mutate(words=filterChineseTerms(words)) %>% # 過濾去不包含中文字的候選詞語
ungroup() %>%
select(id, Source, date, title, words)  # 選擇新聞編號和斷詞結果
#建立語料庫
vc = VCorpus(VectorSource(news.df$words))
#將形式為character string的句子依據詞語之間的空白轉為vector，vector上的單位為詞語
strsplit_space_tokenizer <- function(x)
unlist(strsplit(as.character(x), "[[:space:]]+"))
#建立文件-詞語矩陣，每個元素為每一種詞語出現在一筆文件上的次數
dtm = DocumentTermMatrix(vc,
control=list(tokenize=strsplit_space_tokenizer, wordLengths=c(1, Inf)))
#統計詞語出現的總頻次
term.count = col_sums(dtm)
#統計詞語出現的文件數
term.df = tapply(dtm$v, dtm$j, length)
#統計出現總頻次大於20
cr1 <- term.count > 5
length(which(cr1))
#統計出現系所數不大於1/4系所總數
cr2 <- term.df < 100
length(which(cr2))
cr3 <- !(dtm$dimnames$Terms %in% c("的", "之", "暨", "系", "型",
"一般", "等", "本科", "性",
"和", "在", "化", "大", "一",
"概論", "專題", "基礎", "相關"))
length(which(cr3))
stopwords <- c("的", "在", "是", "都", "了", "也", "很", "會", "有", "呢", "嗎", "就", "但", "所", "我", "不", "到", "要", "於")
cr3 <- !(dtm$dimnames$Terms %in% stopwords)
length(which(cr3))
dim(dtm)
#統計出現總頻次大於20且出現系所數不大於1/4系所總數的詞語數
length(which(cr1 & cr2 & cr3))
dtm$dimnames$Terms[1:10]
dtm$dimnames$Terms[1:12]
dtm$dimnames$Terms[11:30]
dtm$dimnames$Terms[31:100]
#刪除詞語，保留文件-詞語矩陣上出現總頻次大於20且出現系所數不大於1/4系所總數的詞語
dtm1 <- dtm[, cr1&cr2&cr3]
dtm$dimnames$Terms[1:12]
dtm$dimnames$Terms[11:30]
dtm1$dimnames$Terms[1:100]
dtm1$dimnames$Terms[101:200]
#統計出現總頻次大於20且出現系所數不大於1/4系所總數的詞語數
length(which(cr1 & cr2 & cr3))
#刪除詞語，保留文件-詞語矩陣上出現總頻次大於20且出現系所數不大於1/4系所總數的詞語
dtm1 <- dtm[, cr1&cr2&cr3]
#統計每一筆文件上出現的詞語數
doc.termno = row_sums(dtm1)
#是否有沒有詞語的文件
which(doc.termno<=10)
dtm1 <- dtm1[doc.termno>10, ]
dtm1$dimnames$Docs
out <- list()
out$documents <- lapply(1:dtm1$nrow, function (x) matrix(as.integer(c(dtm1$j[dtm1$i==x], dtm1$v[dtm1$i==x])), nrow=2, byrow=TRUE))
out$vocab <- dtm1$dimnames$Terms
news.df1 <- news.df[dtm1$dimnames$Docs, ]
out$meta <- news.df1
stora <- searchK(documents=out$documents, vocab=out$vocab,
K=0, prevalence=~Source,
data=out$meta)
plot.searchK(stora)
View(stora)
stora <- searchK(documents=out$documents, vocab=out$vocab,
K=seq(3, 30, 3), prevalence=~Source,
data=out$meta, init.type = "LDA")
plot.searchK(stora)
View(stora)
stora <- searchK(documents=out$documents, vocab=out$vocab,
K=seq(2, 30, 2), prevalence=~Source,
data=out$meta, init.type = "LDA")
plot.searchK(stora)
